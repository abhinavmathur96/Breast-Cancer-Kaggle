{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre v Post Pruning Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from graphlab import SFrame, cross_validation\n",
    "import pickle as pkl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup base node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class node(object):\n",
    "    def __init__(self, isLeaf, splitting_feature, split_value, data,\n",
    "                 leftChild = None, rightChild = None):\n",
    "        self.isLeaf = isLeaf\n",
    "        self.splitting_feature = splitting_feature\n",
    "        self.split_value = split_value\n",
    "        self.leftChild = leftChild\n",
    "        self.rightChild = rightChild\n",
    "        self.data = data\n",
    "    \n",
    "    def gain(self):\n",
    "        data_left = self.data[self.data[self.splitting_feature]\\\n",
    "                                < self.split_value]\n",
    "        data_right = self.data[self.data[self.splitting_feature]\\\n",
    "                                >= self.split_value]\n",
    "        entropy_left_split = (len(data_left) / float(len(self.data)))\\\n",
    "                                * entropy(data_left)\n",
    "        entropy_right_split = (len(data_right) / float(len(self.data)))\\\n",
    "                                * entropy(data_right)\n",
    "        return entropy(self.data) - entropy_left_split - entropy_right_split\n",
    "    \n",
    "    def split_info(self):\n",
    "        data_left = len(self.data[self.data[self.splitting_feature]\\\n",
    "                                < self.split_value])\n",
    "        data_right = len(self.data[self.data[self.splitting_feature]\\\n",
    "                                >= self.split_value])\n",
    "        ratio_left = data_left / float(len(self.data))\n",
    "        ratio_right = data_right / float(len(self.data))\n",
    "        log_left = np.log2(ratio_left)\n",
    "        log_right = np.log2(ratio_right)\n",
    "        if log_left == -np.inf:\n",
    "            log_left = 0.0\n",
    "        if log_right == -np.inf:\n",
    "            log_right == 0.0\n",
    "        return - (ratio_left * log_left + ratio_right * log_right)\n",
    "    \n",
    "    def gain_ratio(self):\n",
    "        return self.gain() / self.split_info()\n",
    "    \n",
    "    def prediction(self):\n",
    "        if (self.data['diagnosis'] == 'M').sum() >\\\n",
    "            (self.data['diagnosis'] == 'B').sum():\n",
    "            return 'M'\n",
    "        else:\n",
    "            return 'B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Entropy and Best Split Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    '''\n",
    "    Calculate Entropy of the given data\n",
    "    '''\n",
    "    if len(data) != 0:\n",
    "        total_data = len(data)\n",
    "        m_freq = (data['diagnosis'] == 'M').sum() / float(total_data)\n",
    "        b_freq = (data['diagnosis'] == 'B').sum() / float(total_data)\n",
    "        if m_freq == 0:\n",
    "            log2_m = 0.0\n",
    "        else:\n",
    "            log2_m = np.log2(m_freq)\n",
    "        if b_freq == 0:\n",
    "            log2_b = 0.0\n",
    "        else:\n",
    "            log2_b = np.log2(b_freq)\n",
    "        return -(m_freq * log2_m + b_freq * log2_b)\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "def best_splitting_feature_gain(data, features, isContinuous=True,\n",
    "                                annotate=False):\n",
    "    '''\n",
    "    Calculate best splitting feature through Entropy\n",
    "    '''\n",
    "    n = float((data['diagnosis'] == 'M').sum())\n",
    "    p = float((data['diagnosis'] == 'B').sum())\n",
    "    entropy_data = entropy(data)\n",
    "    if annotate:\n",
    "        print \"Entropy of data: %f\" % entropy_data\n",
    "\n",
    "    best_feature = None\n",
    "    best_gain = -np.inf\n",
    "\n",
    "    for feature in features:\n",
    "        if annotate:\n",
    "            print 'Working on ' + feature\n",
    "        if isContinuous:\n",
    "            data_sorted = data[feature].sort()\n",
    "            data_points = [np.mean([data_sorted[i], data_sorted[i + 1]]) \\\n",
    "                            for i in range(len(data_sorted) - 1)]\n",
    "            gain = []\n",
    "            for i in data_points:\n",
    "                #For points <= split\n",
    "                pi_tmp = float(((data['diagnosis'] == 'B') & (data[feature] <= i)).sum())\n",
    "                ni_tmp = float(((data['diagnosis'] == 'M') & (data[feature] <= i)).sum())\n",
    "                left_split = float((pi_tmp + ni_tmp) / (p + n))\n",
    "                log_left = np.log2(left_split)\n",
    "                if log_left == -np.inf:\n",
    "                    log_left = 0.0\n",
    "                left_split_info = float(left_split * log_left)\n",
    "                gain.append(((pi_tmp + ni_tmp) / (p + n)) * entropy(data[data[feature] <= i]))\n",
    "\n",
    "                #For points > split\n",
    "                pi_tmp = float(((data['diagnosis'] == 'B') & (data[feature] > i)).sum())\n",
    "                ni_tmp = float(((data['diagnosis'] == 'M') & (data[feature] > i)).sum())\n",
    "                right_split = float((pi_tmp + ni_tmp) / (p + n))\n",
    "                log_right = np.log2(right_split)\n",
    "                if log_right == -np.inf:\n",
    "                    log_right = 0.0\n",
    "                right_split_info = float(right_split * log_right)\n",
    "                gain.append(((pi_tmp + ni_tmp) / (p + n)) * entropy(data[data[feature] > i]))\n",
    "\n",
    "            gain_split = entropy_data - float(sum(gain))\n",
    "            gain_ratio = gain_split / (-(left_split_info + right_split_info))\n",
    "            if annotate:\n",
    "                print 'Gain Ratio from %s: %f' % (feature, gain_ratio)\n",
    "            if gain_ratio > best_gain:\n",
    "                best_gain = gain_ratio\n",
    "                best_feature = feature\n",
    "        else:\n",
    "            print 'TODO: Implement discrete valued Information Gain'\n",
    "\n",
    "    return best_feature, np.median(data[best_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking mistakes by minority class (also used for checking pure nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    '''\n",
    "    Counts basic error in current split based on minority class\n",
    "    '''\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Count the number of B's (benign tumors)\n",
    "    num_b = (labels_in_node == 'B').sum()\n",
    "\n",
    "    # Count the number of M's (malignant tumors)\n",
    "    num_m = (labels_in_node == 'M').sum()\n",
    "\n",
    "    # Return the number of mistakes that the majority classifier makes.\n",
    "    return min(num_b, num_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create pre-pruned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pre_pruned(data, features, current_depth=0, max_depth=10,\n",
    "                        threshold=10):\n",
    "    '''\n",
    "    Create a decision tree with pre-pruning similar to ID3 or C4.5\n",
    "    '''\n",
    "    target_values = data['diagnosis']\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    # Stopping Condition 1\n",
    "    # Stop at pure nodes\n",
    "    if intermediate_node_num_mistakes(data['diagnosis']) == 0:\n",
    "        print 'Pure node reached'\n",
    "        return node(isLeaf = True, splitting_feature = None,\n",
    "                        split_value = None, data = data)\n",
    "    \n",
    "    # Stopping Condition 2\n",
    "    # Stop when number of datapoints fall below a threshold\n",
    "    if len(data) <= threshold:\n",
    "        print 'Less than %d datapoints left' % threshold\n",
    "        return node(isLeaf = True, splitting_feature = None,\n",
    "                        split_value = None, data = data)\n",
    "    \n",
    "    # Stopping Condition 3\n",
    "    # Stop after specified tree depth\n",
    "    if current_depth >= max_depth:\n",
    "        print 'Max depth reached (%d)' % max_depth\n",
    "        return node(isLeaf = True, splitting_feature = None,\n",
    "                        split_value = None, data = data)\n",
    "    \n",
    "    # Finding best split through gain ratio\n",
    "    splitting_feature, split_value = best_splitting_feature_gain(data, features, 'diagnosis')\n",
    "    left_split = data[data[splitting_feature] < split_value]\n",
    "    right_split = data[data[splitting_feature] >= split_value]\n",
    "    print 'Split on feature %s at %f' % (splitting_feature, split_value)\n",
    "\n",
    "    # Create a leaf node if split is perfect\n",
    "    if len(right_split) == 0:\n",
    "        print 'Creating leaf node for left data'\n",
    "        return node(isLeaf = True, splitting_feature = None,\n",
    "                        split_value = None, data = left_split)\n",
    "    if len(left_split) == 0:\n",
    "        print 'Creating leaf node for right data'\n",
    "        return node(isLeaf = True, splitting_feature = None,\n",
    "                        split_value = None, data = right_split)\n",
    "    \n",
    "    # Recurse on left and right subtrees\n",
    "    left_tree = create_pre_pruned(left_split, features, current_depth+1, max_depth)\n",
    "    right_tree = create_pre_pruned(right_split, features, current_depth+1, max_depth)\n",
    "\n",
    "    return node(\n",
    "        isLeaf = False,\n",
    "        splitting_feature = splitting_feature,\n",
    "        split_value = split_value,\n",
    "        leftChild = left_tree,\n",
    "        rightChild = right_tree,\n",
    "        data = data\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create unbounded tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_unbounded(data, features, current_depth=0):\n",
    "    '''\n",
    "    Create a decison tree without any checks.\n",
    "    It is inadvisable to use this without prune().\n",
    "    '''\n",
    "    target_values = data['diagnosis']\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "\n",
    "    # Stop at pure nodes\n",
    "    if intermediate_node_num_mistakes(data['diagnosis']) == 0:\n",
    "        print 'Pure node reached'\n",
    "        return node(isLeaf = True, splitting_feature = None,\n",
    "                        split_value = None, data = data)\n",
    "    \n",
    "    # Finding best split through gain ratio\n",
    "    splitting_feature, split_value = best_splitting_feature_gain(data, features, 'diagnosis')\n",
    "    left_split = data[data[splitting_feature] < split_value]\n",
    "    right_split = data[data[splitting_feature] >= split_value]\n",
    "    print 'Split on feature %s at %f' % (splitting_feature, split_value)\n",
    "\n",
    "    # Create a leaf node if split is perfect\n",
    "    if len(left_split) == len(data):\n",
    "        print 'Creating leaf node'\n",
    "        return node(isLeaf = True, splitting_feature = None,\n",
    "                        split_value = None, data = left_split)\n",
    "    if len(right_split) == len(data):\n",
    "        print 'Creating leaf node'\n",
    "        return node(isLeaf = True, splitting_feature = None,\n",
    "                        split_value = None, data = right_split)\n",
    "    \n",
    "    # Recurse on left and right subtrees\n",
    "    left_tree = create_unbounded(left_split, features, current_depth+1)\n",
    "    right_tree = create_unbounded(right_split, features, current_depth+1)\n",
    "\n",
    "    return node(\n",
    "        isLeaf = False,\n",
    "        splitting_feature = splitting_feature,\n",
    "        split_value = split_value,\n",
    "        leftChild = left_tree,\n",
    "        rightChild = right_tree,\n",
    "        data = data\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to post-prune unbounded tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def prune(tree, test_data):\n",
    "    '''\n",
    "    Prunes a pre-built tree. Returns pruned tree and least error encountered.\n",
    "    '''\n",
    "    tree_copy = copy.deepcopy(tree)\n",
    "    bfs = [tree_copy]\n",
    "    bfs_iter = 0\n",
    "\n",
    "    while(True):\n",
    "        if bfs_iter == len(bfs):\n",
    "            break\n",
    "        if bfs[bfs_iter].leftChild != None:\n",
    "            bfs.append(bfs[bfs_iter].leftChild)\n",
    "        if bfs[bfs_iter].rightChild != None:\n",
    "            bfs.append(bfs[bfs_iter].rightChild)\n",
    "        bfs_iter += 1\n",
    "    \n",
    "    least_error = evaluate_classification_error(tree_copy, test_data)\n",
    "    snips = 0\n",
    "\n",
    "    bfs.reverse()\n",
    "    for i in bfs:\n",
    "        if i != None:\n",
    "            i.isLeaf = True\n",
    "            error_partial = evaluate_classification_error(tree_copy, test_data)\n",
    "            if error_partial <= least_error:\n",
    "                least_error = error_partial\n",
    "                snips += 1\n",
    "            else:\n",
    "                i.isLeaf = False\n",
    "    print 'Made %d snips' % snips\n",
    "    return tree_copy, least_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    '''\n",
    "    Return number of nodes in the tree\n",
    "    '''\n",
    "    if tree.isLeaf:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree.leftChild) + count_nodes(tree.rightChild)\n",
    "\n",
    "def classify(tree, x, annotate=False):\n",
    "    '''\n",
    "    Returns prediction for a row\n",
    "    '''\n",
    "    # if the node is a leaf node.\n",
    "    if tree.isLeaf:\n",
    "        if annotate:\n",
    "            print \"At leaf, predicting %s\" % tree.prediction()\n",
    "        return tree.prediction()\n",
    "    else:\n",
    "        # split on feature.\n",
    "        feature_value = x[tree.splitting_feature]\n",
    "        split_value = tree.split_value\n",
    "        if annotate:\n",
    "            print \"Split on %s = %s\" % (tree.splitting_feature, feature_value)\n",
    "        if feature_value < split_value:\n",
    "            return classify(tree.leftChild, x, annotate)\n",
    "        else:\n",
    "            return classify(tree.rightChild, x, annotate)\n",
    "\n",
    "def evaluate_classification_error(tree, data, annotate=False):\n",
    "    '''\n",
    "    Returns classification error\n",
    "    '''\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x, annotate=annotate))\n",
    "\n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    return (data['diagnosis'] != prediction).sum() / float(len(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to abhinav14csu009@ncuindia.edu and will expire on August 16, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\Abhinav\\AppData\\Local\\Temp\\graphlab_server_1493900831.log.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Abhinav\\Desktop\\DW&M\\Breast-Cancer-Kaggle\\data.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Abhinav\\Desktop\\DW&M\\Breast-Cancer-Kaggle\\data.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 569 lines in 0.044021 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 569 lines in 0.044021 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of benign tumors              : 0.518181818182\n",
      "Percentage of malignant tumors           : 0.481818181818\n",
      "Total number of tumors in new dataset    : 440\n"
     ]
    }
   ],
   "source": [
    "data = SFrame.read_csv('data.csv', column_type_hints = [str, str] + [float]*30)\n",
    "benign_raw, malign_raw = data[data['diagnosis'] == 'B'], data[data['diagnosis'] == 'M']\n",
    "percentage = len(malign_raw) / float(len(benign_raw))\n",
    "benign = benign_raw.sample(percentage)\n",
    "malign = malign_raw\n",
    "data = benign.append(malign)\n",
    "data = cross_validation.shuffle(data)\n",
    "\n",
    "print \"Percentage of benign tumors              :\", len(benign) / float(len(data))\n",
    "print \"Percentage of malignant tumors           :\", len(malign) / float(len(data))\n",
    "print \"Total number of tumors in new dataset    :\", len(data)\n",
    "train_data, test_data = data.random_split(.8)\n",
    "target = 'diagnosis'\n",
    "features = data.column_names()[2:]\n",
    "\n",
    "# Test function. Should return perimeter_worst and its median\n",
    "# print best_splitting_feature_gain(data, data.column_names()[2:], 'diagnosis', annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-pruned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (363 data points).\n",
      "Split on feature perimeter_worst at 105.000000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (181 data points).\n",
      "Split on feature concave points_worst at 0.069870\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (90 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (91 data points).\n",
      "Split on feature concave points_worst at 0.089780\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (45 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (46 data points).\n",
      "Split on feature concave points_worst at 0.115900\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (23 data points).\n",
      "Split on feature concave points_mean at 0.034830\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature radius_worst at 13.740000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (23 data points).\n",
      "Split on feature concave points_worst at 0.142400\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature radius_mean at 12.620000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Split on feature area_se at 23.315000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (182 data points).\n",
      "Split on feature perimeter_worst at 133.500000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (90 data points).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhinav\\desktop\\dw&m\\gl-env\\lib\\site-packages\\ipykernel_launcher.py:57: RuntimeWarning: divide by zero encountered in log2\n",
      "c:\\users\\abhinav\\desktop\\dw&m\\gl-env\\lib\\site-packages\\ipykernel_launcher.py:64: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature concave points_mean at 0.061845\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (45 data points).\n",
      "Split on feature texture_mean at 20.010000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (22 data points).\n",
      "Split on feature perimeter_worst at 112.550000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature smoothness_worst at 0.125400\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature perimeter_worst at 116.600000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (23 data points).\n",
      "Split on feature fractal_dimension_se at 0.002575\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Split on feature concave points_mean at 0.053330\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (45 data points).\n",
      "Split on feature concave points_mean at 0.081720\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (22 data points).\n",
      "Split on feature texture_worst at 27.665000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature area_mean at 793.200000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (23 data points).\n",
      "Split on feature concave points_mean at 0.090520\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature texture_mean at 20.520000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Less than 10 datapoints left\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (92 data points).\n",
      "Pure node reached\n",
      "Pre-pruned model took 6681.48399997 seconds\n",
      "\n",
      "Cannot read pre-pruned model binary file\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a pre-pruned tree and save that model for later use\n",
    "time_pre = time.time()\n",
    "model_pre_pruned = create_pre_pruned(train_data, features)\n",
    "print 'Pre-pruned model took {} seconds'.format(time.time() - time_pre)\n",
    "with open('model_pre_pruned.pkl', 'wb') as output:\n",
    "    pkl.dump(model_pre_pruned, output, -1)\n",
    "\n",
    "# del model_pre_pruned\n",
    "try:\n",
    "    with open('model_pre_pruned.pkl', 'rb') as inp:\n",
    "        tmp = pkl.load(inp)\n",
    "except Exception:\n",
    "    print '\\nCannot read pre-pruned model binary file'\n",
    "print '==================================================='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbounded tree (Non-pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (363 data points).\n",
      "Split on feature perimeter_worst at 105.000000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (181 data points).\n",
      "Split on feature concave points_worst at 0.069870\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (90 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (91 data points).\n",
      "Split on feature concave points_worst at 0.089780\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (45 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (46 data points).\n",
      "Split on feature concave points_worst at 0.115900\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (23 data points).\n",
      "Split on feature concave points_mean at 0.034830\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature radius_worst at 13.740000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Split on feature texture_mean at 17.395000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Split on feature radius_mean at 13.440000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (2 data points).\n",
      "Split on feature radius_mean at 13.570000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (23 data points).\n",
      "Split on feature concave points_worst at 0.142400\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature radius_mean at 12.620000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Split on feature radius_mean at 13.400000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Split on feature radius_mean at 13.710000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (2 data points).\n",
      "Split on feature radius_mean at 14.155000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Split on feature area_se at 23.315000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Split on feature texture_worst at 24.175000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Split on feature compactness_mean at 0.121800\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (2 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (182 data points).\n",
      "Split on feature perimeter_worst at 133.500000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (90 data points).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhinav\\desktop\\dw&m\\gl-env\\lib\\site-packages\\ipykernel_launcher.py:57: RuntimeWarning: divide by zero encountered in log2\n",
      "c:\\users\\abhinav\\desktop\\dw&m\\gl-env\\lib\\site-packages\\ipykernel_launcher.py:64: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature concave points_mean at 0.061845\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (45 data points).\n",
      "Split on feature texture_mean at 20.010000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (22 data points).\n",
      "Split on feature perimeter_worst at 112.550000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature smoothness_worst at 0.125400\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Split on feature texture_mean at 15.440000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Split on feature radius_mean at 14.420000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (2 data points).\n",
      "Split on feature radius_mean at 15.360000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature perimeter_worst at 116.600000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Split on feature perimeter_worst at 114.200000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (2 data points).\n",
      "Split on feature radius_mean at 15.610000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Split on feature perimeter_worst at 115.900000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (2 data points).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhinav\\desktop\\dw&m\\gl-env\\lib\\site-packages\\ipykernel_launcher.py:64: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature radius_mean at 15.875000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Split on feature perimeter_worst at 119.000000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Split on feature radius_mean at 15.120000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (2 data points).\n",
      "Split on feature radius_mean at 15.810000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Split on feature smoothness_mean at 0.089230\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (2 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (23 data points).\n",
      "Split on feature fractal_dimension_se at 0.002575\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Split on feature concave points_mean at 0.053330\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Split on feature fractal_dimension_se at 0.003954\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (45 data points).\n",
      "Split on feature concave points_mean at 0.081720\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (22 data points).\n",
      "Split on feature texture_worst at 27.665000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature area_mean at 793.200000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Split on feature compactness_mean at 0.129900\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (2 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (23 data points).\n",
      "Split on feature concave points_mean at 0.090520\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature texture_mean at 20.520000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (5 data points).\n",
      "Split on feature texture_mean at 17.570000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (2 data points).\n",
      "Split on feature radius_mean at 16.200000\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (3 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12 data points).\n",
      "Pure node reached\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (92 data points).\n",
      "Pure node reached\n",
      "Unbounded model took 14020.4659998 seconds\n",
      "\n",
      "Cannot read unbounded model binary file\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "# Create an unbounded tree for post-pruning later. Save the model\n",
    "time_unbounded = time.time()\n",
    "model_unbounded = create_unbounded(train_data, features)\n",
    "print 'Unbounded model took {} seconds'.format(time.time() - time_unbounded)\n",
    "with open('model_unbounded.pkl', 'wb') as output:\n",
    "    pkl.dump(model_unbounded, output, -1)\n",
    "\n",
    "# del model_unbounded\n",
    "try:\n",
    "    with open('model_unbounded.pkl', 'rb') as inp:\n",
    "        tmp = pkl.load(inp)\n",
    "except Exception:\n",
    "    print '\\nCannot read unbounded model binary file'\n",
    "print '==================================================='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune the unbounded tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made 75 snips\n",
      "Pruning model took 837.286000013 seconds\n",
      "Test error of pruned tree: 0.025974\n",
      "\n",
      "Cannot read post pruned model binary file\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "# Now prune the unbounded tree wrt error in test_data and save this too\n",
    "time_post_pruned = time.time()\n",
    "model_post_pruned, least_error = prune(model_unbounded, test_data)\n",
    "print 'Pruning model took {} seconds'.format(time.time() - time_post_pruned)\n",
    "print 'Test error of pruned tree: %f' % least_error\n",
    "with open('model_post_pruned.pkl', 'wb') as output:\n",
    "    pkl.dump(model_post_pruned, output, -1)\n",
    "\n",
    "# del model_post_pruned\n",
    "try:\n",
    "    with open('model_post_pruned.pkl', 'rb') as inp:\n",
    "        tmp = pkl.load(inp)\n",
    "except Exception:\n",
    "    print '\\nCannot read post pruned model binary file'\n",
    "print '==================================================='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking our tree for a test drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Test Data Row----------------------\n",
      "{'area_se': 17.86, 'concavity_mean': 0.03809, 'compactness_worst': 0.104, 'radius_mean': 11.71, 'perimeter_worst': 84.42, 'fractal_dimension_se': 0.001671, 'smoothness_se': 0.006905, 'area_worst': 521.5, 'compactness_mean': 0.06141, 'fractal_dimension_worst': 0.07097, 'symmetry_se': 0.01897, 'smoothness_mean': 0.09774, 'id': '874373', 'texture_se': 0.7655, 'fractal_dimension_mean': 0.06095, 'perimeter_mean': 74.68, 'concave points_mean': 0.03239, 'concavity_worst': 0.1521, 'smoothness_worst': 0.1323, 'concave points_worst': 0.1099, 'concavity_se': 0.01978, 'symmetry_mean': 0.1516, 'area_mean': 420.3, 'texture_worst': 21.39, 'symmetry_worst': 0.2572, 'concave points_se': 0.01185, 'texture_mean': 17.19, 'radius_worst': 13.01, 'radius_se': 0.2451, 'diagnosis': 'B', 'perimeter_se': 1.742, 'compactness_se': 0.008704}\n",
      "---------------------------------------------------\n",
      "Split on perimeter_worst = 84.42\n",
      "Split on concave points_worst = 0.1099\n",
      "Split on concave points_worst = 0.1099\n",
      "Split on concave points_worst = 0.1099\n",
      "At leaf, predicting B\n",
      "Post-pruned model says: B\n",
      "Split on perimeter_worst = 84.42\n",
      "Split on concave points_worst = 0.1099\n",
      "Split on concave points_worst = 0.1099\n",
      "Split on concave points_worst = 0.1099\n",
      "Split on concave points_mean = 0.03239\n",
      "Split on radius_worst = 13.01\n",
      "At leaf, predicting B\n",
      "Pre-pruned model says: B\n"
     ]
    }
   ],
   "source": [
    "# Let's try classifying a random row from the test_data\n",
    "rand_choice = np.random.randint(0, len(test_data))\n",
    "print '----------------Test Data Row----------------------'\n",
    "print test_data[rand_choice]\n",
    "print '---------------------------------------------------'\n",
    "# What does the post_pruned model say?\n",
    "print 'Post-pruned model says: %s' % classify(model_post_pruned, test_data[rand_choice], annotate=True)\n",
    "# What does the pre_pruned model say?\n",
    "print 'Pre-pruned model says: %s' % classify(model_pre_pruned, test_data[rand_choice], annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating classification error\n",
    "#### Error here is given by $$\\frac{\\text{total no of records misclassified}}{\\text{total no of records}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruned: 0.052273\n",
      "Post-pruned: 0.045455\n",
      "Unbounded: 0.045455\n"
     ]
    }
   ],
   "source": [
    "# What is the overall classification error?\n",
    "print 'Pre-pruned: %f' % evaluate_classification_error(model_pre_pruned, data)\n",
    "print 'Post-pruned: %f' % evaluate_classification_error(model_post_pruned, data)\n",
    "print 'Unbounded: %f' % evaluate_classification_error(model_unbounded, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see from the result that the pre-pruned model has a lower error (~4.5%) than the post-pruned model (~5.3%)\n",
    "### This is because the post-pruned model tends to generalize better\n",
    "Also note that the unbounded model has the same error rate as the post-pruned. This is infact an incorrect result due to the fact that the pruning algorithm does not create a deep copy of the passed parameter and hence all changes made in the function are made in the global variable. Unbounded model is in fact the post-pruned model after prune() is called."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
